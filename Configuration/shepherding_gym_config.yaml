# Configuration file
# In all scripts numpy is imported as np

seed: 1

DampedDoubleIntegrators:
  N: 2
  state_dim: 4

  x0_mode: Random
  x0_shape: circle
  max_initial_radius: 25

  D: 1
  damping: 3
  id: 'Targets'

SimpleIntegrators:
  N: 1
  state_dim: 2
  x0_mode: Random
  x0_shape: circle
  max_initial_radius: 25

  params_mode: Random
  params_names: ["D"]
  params_limits: {D: [[0,0], [0,0]]}
  id: 'Herders'

integrator:
  dt: 0.01

simulator:
  T: 1

environment:
  dimensions: [50, 50]
  goal_pos: [0, 0]
  goal_radius: 5
  final_goal_pos : [20, 20]
  num_steps : 2000
  start_step: 2000

renderer:
  background_color: 'white'
  agent_colors: ['magenta', 'blue']
  agent_shapes: ['circle', 'diamond']
  agent_size: [1, 1]
  render_mode: 'pygame'
  render_dt: 0.01

logger:
  activate: False         # True to save logger, False otherwise
  log_freq: 1             # Print every log_freq steps information (0: never print)
  save_freq: 1            # Save every save_freq steps information (0: never save)
  comment_enable: False   # Add initial and final comments to the logger about the experiment
  log_path: .\logs        # Path where logger should be saved
  log_name: ''            # String appended to date in the name of the file

RepulsionLongRange:
  strength: 15
  max_distance: 5
  p: 2

RepulsionShortRange:
  strength: 10
  max_distance: 1.5
  p: 4

AttractionLongRange:
  strength: -0 #-2
  max_distance: 10
  p: 2

TargetInteraction:
  strength_attr: 4
  strength_rep: 10
  max_distance: 15
  p_attr: 2
  p_rep: 4
  is_attractive: False

Gym:
  num_episodes : 10
  action_bound : 12
  reward_gain : 1
  num_acts: 5  # For DQN only

  single_agent_env:
    obs_style : "PPO"
    k_1 : 1
    k_2 : 1
    k_3 : 1
    k_4 : 1

  marl_wrapper:
    update_frequency    : 1
    num_closest_targets : 2
    num_closest_herders : 1
    low_level_policy    : "PPO"
    obs_style   : "DQN"

  # Network architecture configuration
  high_level_network:
    hidden_sizes: [ 256, 128 ]
    activation: "ReLU"

