# Configuration file
# In all scripts numpy is imported as np

seed: 1

DampedDoubleIntegrators:
  id: 'Targets'
  N: 10
  state_dim: 4
  input_dim: 2
  # lim: ['inf', 'inf', 'inf', 'inf']

  initial_conditions:
    mode: random
    random:
      shape: circle
      circle:
        max_radius: 25

  parameters:
    mode: generate
    generate:
      D: 0.1
      damping: 3



SimpleIntegrators:
  id: 'Herders'
  N: 5
  state_dim: 2

  initial_conditions:
    mode: random
    random:
      shape: circle
      circle:
        max_radius: 25

integrator:
  dt: 0.01

simulator:
  T: 100

environment:
  dimensions: [50, 50]
  goal_pos: [0, 0]
  goal_radius: 5
  final_goal_pos : [0, 0]
  num_steps : 2000
  start_step: 2000

renderer:
  background_color: 'white'
  agent_colors: ['magenta', 'blue']
  agent_shapes: ['circle', 'diamond']
  agent_size: [1, 1]
  render_mode: 'pygame'
  render_dt: 0.01

logger:
  activate: True         # True to save logger, False otherwise
  log_freq: 0             # Print every log_freq steps information (0: never print)
  save_freq: 1            # Save every save_freq steps information (0: never save)
  comment_enable: False   # Add initial and final comments to the logger about the experiment
  log_path: .\logs        # Path where logger should be saved
  log_name: ''            # String appended to date in the name of the file

RepulsionLongRange:
  strength: 15
  max_distance: 5
  p: 2

RepulsionShortRange:
  strength: 10
  max_distance: 1.5
  p: 4

AttractionLongRange:
  strength: -0 #-2
  max_distance: 10
  p: 2

TargetInteraction:
  strength_attr: 4
  strength_rep: 10
  max_distance: 15
  p_attr: 2
  p_rep: 4
  is_attractive: False

Gym:
  num_episodes : 10
  action_bound : 12
  reward_gain : 1
  num_acts: 5  # For DQN only

  single_agent_env:
    obs_style : "PPO"
    k_1 : 1
    k_2 : 1
    k_3 : 1
    k_4 : 1

  marl_wrapper:
    update_frequency    : 1
    num_closest_targets : 2
    num_closest_herders : 1
    low_level_policy    : "PPO"
    obs_style   : "DQN"

  # Network architecture configuration
  high_level_network:
    hidden_sizes: [ 256, 128 ]
    activation: "ReLU"

